---
title: "Exploring how best to get robust coefficients of overlap with uncertainty"
author: "David"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```

```{r load packages}
#Load necessary packages
library(lubridate)
library(tidyverse)
library(overlap)
library(circular)
library(CircStats)
library(adehabitatLT)
library(here)
```


I'll be working with the Spring GPS and camera data. 
```{r import data}
# Import camera data from spring
spring_cams <-read_csv(here("data/BIR_Spring_Cams.csv")) 

#Import gps data from spring
spring_gps <- read.csv(here("data/BIRSpringGPS.csv")) 
```

Pre-processing steps
```{r}
dates <- as.POSIXct(strptime(as.character(spring_gps$Fix_DateTime),"%Y/%m/%d %H:%M", tz="America/New_York"))
traj_spring <- as.ltraj(xy = spring_gps[,c("X","Y")], date = dates,
                             id = spring_gps$Indiv_ID, typeII= TRUE, burst = spring_gps$Indiv_ID)

ld_spring <- ld(traj_spring) #ld is a function from the adehabitatlt package that converts from ltraj object back to dataframe

spring_df<-ld_spring %>% 
  mutate(dtmin=dt/60,       #creates a new column for amount of time between points in minutes
         dthr=dt/3600,      #creates a new column for time between points in hours
         meterph=round(dist/dthr, digits=0))%>% #calculates movement rate in meters per hour, rounds to nearest
  dplyr::select(id, x, y, date, dist,dt, dtmin, dthr, meterph) #selects columns of interest
```
Data processed for all 53 pigs.  

Function to generate pseudo-observations
```{r}
gen_rand_times.mod <- function(N, st, et) {
  dt <- as.numeric(difftime(et, st, unit="sec")) #calculates the interval in seconds between the start time and end time of the interval
  ev <- sort(runif(N, 0, dt)) #generates N random values within that interval and sorts them from least to greatest
  rt <- st + ev } # adds the generate times in seconds to the start value to create a vector of times between the start and the end
```

Now generate pseudo-observations for each pig, calculate coefficient of overlap again the spring camera data using Dhat4 option, and store the kernel density info.
```{r eval=F}
ids<-unique(spring_df$id)
overlap_df<-data.frame(ids=ids, overlap=NA)
kernel_df<-data.frame()
for(i in seq_along(ids)){
  tmp_res<-list()
  tmp<-spring_df %>% filter(id==ids[[i]])
  for(j in 1:nrow(tmp)){
  if(tmp$dtmin[j]>19 & tmp$dtmin[j]<41 & is.na(tmp$dtmin[j])==FALSE){
    tmp_res[[j]]<-gen_rand_times.mod(N=tmp$meterph[j], st=tmp$date[j], et = tmp$date[j+1])
  }
}

tmp_df <-plyr::ldply(tmp_res, data.frame) #takes all the results from the list and binds them into a single dataframe

tmp_df<-tmp_df %>% 
  rename(datetime=`X..i..`) %>%  #renames default name with "datetime"
  mutate(Indiv_ID=rep("data", times=nrow(tmp_df)), #creates a new column for the animal ID repeated for the length of the dataframe
         time=format(ymd_hms(datetime), "%H:%M:%S")) #extracts just the time of each observation from the datetime

tmp_df<-tmp_df %>% 
  mutate(fractime=hms(time)/hms("24:00:00"), #calculates the fraction of the diel cycle based on the time
         timeRad=fractime*2*pi)#multiplies to get radians to make data circular

# produce overlap coefficient and store in dataframe
tmp_overlap<-overlapEst(spring_cams$timeRad, tmp_df$timeRad, type = "Dhat4") 
overlap_df[ids==ids[[i]], 2]<-tmp_overlap

# produce activity kernel and store in dataframe
tmp_kernel<-densityPlot(tmp_df$timeRad, extend=NULL)
tmp_kernel<-tmp_kernel %>% 
  rename(time_rad=x, kern_density=y) %>% 
  mutate(id=ids[[i]])

kernel_df<-bind_rows(kernel_df, tmp_kernel)
cat(paste(i, "out of 53 \n "))
} 
```

```{r}
#save this output to file
# write_csv(overlap_df, "output/spring_overlap_coefficients.csv")
# write_csv(kernel_df, "output/spring_gps_activity_kernels.csv")

# bring back kernel_df from chunk above that wasn't run to save time
kernel_df<-read_csv(here("output/spring_gps_activity_kernels.csv"))
```


Now average over all individuals to get a single activity kernel
```{r}
kern_avg<-kernel_df %>% 
  group_by(time_rad) %>% 
  summarise(avg_dens=mean(kern_density))

plot(kern_avg$time_rad, kern_avg$avg_dens)
```
  
These values don't add up to 1 so need to be normalized but the plot shouldn't change, just the y-axis. If relative activity is the goal I'm not sure it even matters.  


If it'd be interesting to check out the confidence intervals for this averaged activity kernel, I can make this by giving each pig equal weight.
```{r}
kern_stats<-kernel_df %>% 
  group_by(time_rad) %>% 
  summarise(mean_activity=mean(kern_density),
            sd_k=sd(kern_density),
            n_k=n()) %>%
  mutate(se_k=sd_k/sqrt(n_k),
         lower_ci=mean_activity-qt(1-(0.05/2), n_k-1)*se_k,
         upper_ci=mean_activity+qt(1-(0.05/2), n_k-1)*se_k) 
  
kern_stats %>% 
  ggplot(aes(time_rad, mean_activity))+
  geom_line()+
  geom_ribbon(aes(ymin=lower_ci, ymax=upper_ci), alpha=0.2)+
  ggtitle("Average kernel density for all pigs gps data in the spring")
```

Lets try to add the camera data as well
```{r}
cam_kern<-densityPlot(spring_cams$timeRad, extend=NULL)
cam_kern<-cam_kern %>% 
   rename(time_rad=x, kern_density=y) %>% 
  mutate(source="camera")

kern_avg<-kern_avg %>% mutate(source="gps")
kern_avg<-kern_avg %>% rename(kern_density=avg_dens)

ggdf<-rbind(cam_kern, kern_avg)


ggdf %>% 
  ggplot(aes(time_rad, kern_density, color=source))+
  geom_line(size=1)+
  labs(x='hour of day')+
  ggtitle('FL Spring Camera Activity vs Spring GPS Activity')

```

Now lets mess with coefficient of overlap values.  

I'm curious in comparing the different ways we can produce this metric.  

First lets calculate it using the group-averaged (but not bootstrapped) gps kernel vs the camera kernel.  

To do this I have to first normalize the kernel density vectors to sum to 1 and then I can calculate the area of coverage under the overlap of the two activity curves, or another way to think about it is that the coefficient of overlap is the sum of the parallel minima of each density function.
```{r}
# normalize camera data
cam_kern$norm<-cam_kern$kern_density/sum(cam_kern$kern_density)

# normalize gps data
kern_avg$norm<-kern_avg$kern_density/sum(kern_avg$kern_density)

# now take the area under the curve
sum(pmin(cam_kern$norm, kern_avg$norm))

```
So here's the point estimate, but not sure the best way to get CI.  

When I created activity curves for each pig, I also calculated coefficients of overlap. Lets compare those estimates to this:

```{r}
# write_csv(overlap_df, "output/spring_overlap_coefficients.csv")
overlap_df<-read_csv(here("output/spring_overlap_coefficients.csv"))

mean(overlap_df$overlap)


```

This is only using one estimate from each pig. Now to see the coverage of the uncertainity by making a CI.


```{r}

lower<-mean(overlap_df$overlap)-qt(1-(0.05/2), nrow(overlap_df))*(sd(overlap_df$overlap)/sqrt(nrow(overlap_df)))
upper<-mean(overlap_df$overlap)+qt(1-(0.05/2), nrow(overlap_df))*(sd(overlap_df$overlap)/sqrt(nrow(overlap_df)))

print(c(lower, upper))

```

So if I understand this correctly, these two ways don't match, which isn't necessarily a problem.

***
  

Now lets try the second approach where I bootstrap individual pigs many times and calculate coefficient with cam data each time. I'll resample 500 random pigs.

```{r eval=FALSE}

ids<-unique(spring_df$id)
bootstrap_overlaps<-vector()
iter<-500

for(i in 1:iter){
  tmp_res<-list()
  tmp_id<-sample(ids, 1)
  tmp<-spring_df %>% filter(id==tmp_id)
  for(j in 1:nrow(tmp)){
    if(tmp$dtmin[j]>19 & tmp$dtmin[j]<41 & is.na(tmp$dtmin[j])==FALSE){
      tmp_res[[j]]<-gen_rand_times.mod(N=tmp$meterph[j], st=tmp$date[j], et = tmp$date[j+1])
    }
  }
  
  tmp_df <-plyr::ldply(tmp_res, data.frame) #takes all the results from the list and binds them into a single dataframe
  
  tmp_df<-tmp_df %>% 
    rename(datetime=`X..i..`) %>%  #renames default name with "datetime"
    mutate(Indiv_ID=rep("data", times=nrow(tmp_df)), #creates a new column for the animal ID repeated for the length of the dataframe
           time=format(ymd_hms(datetime), "%H:%M:%S")) #extracts just the time of each observation from the datetime
  
  tmp_df<-tmp_df %>% 
    mutate(fractime=hms(time)/hms("24:00:00"), #calculates the fraction of the diel cycle based on the time
           timeRad=fractime*2*pi)#multiplies to get radians to make data circular
  
  # produce overlap coefficient and store in vector
  tmp_overlap<-overlapEst(spring_cams$timeRad, tmp_df$timeRad, type = "Dhat4") 
  bootstrap_overlaps[[i]]<-tmp_overlap
  
  
  cat(paste(i, "out of 500 \n "))
} 

write_csv(as.data.frame(bootstrap_overlaps), here("output/bootstrap_overlap_example.csv"))

```

Running 500 iterations took about 3-4 hours (on my mediocre laptop).  

Unfortunately, the sampling distribution of estimates is not bell-shaped, so this might not be enough.
```{r}
b_over<-read_csv(here("output/bootstrap_overlap_example.csv"))
hist(b_over$bootstrap_overlaps)

```






